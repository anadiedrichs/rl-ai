{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-FrozenLake.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anadiedrichs/rl-ai/blob/master/1_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGF1C4uuC-2",
        "colab_type": "text"
      },
      "source": [
        "# Laboratorio #1 de Aprendizaje por refuerzo - Inteligencia Artificial - DISI 2019\n",
        "\n",
        "**Realizado por: Ing. Ana Laura Diedrichs**\n",
        "\n",
        "Consultas: lunes 19 hs en sala consulta de sistemas.\n",
        "\n",
        "Contacto por otros horarios de consulta o dudas: \n",
        "* Email: ana.diedrichs@frm.utn.edu.ar\n",
        "* Telegram  @anadiedrichs\n",
        "* en el grupo de la materia por dudas generales\n",
        "\n",
        "## Objetivos del laboratorio:\n",
        "\n",
        "* Mediante un problema de juguete evaluar las diferencias entre el comportamiento aleatorio de un agente y su comportamiento con aprendizaje por refuerzo\n",
        "* Comprensión del funcionamiento del algoritmo de iteración de valores\n",
        "* Aplicar aprendizaje por refuerzo a un entorno sencillo.\n",
        "* Ser una entrada en calor o introducción del uso del entorno colab.research.google.com y la librería Open AI Gym\n",
        "* Fomentar la aplicación de conceptos teóricos aprendidos al laboratorio.\n",
        "\n",
        "## Pre-requisitos o pre-condiciones\n",
        "* Tener una cuenta google (gmail)\n",
        "* Tener instalado el navegador google chrome \n",
        "* Contar con conectividad a Internet \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCcPp_g1Zwr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHxwqf3lRrrY",
        "colab_type": "text"
      },
      "source": [
        "## Entrega y uso del laboratorio\n",
        "\n",
        "**USO**\n",
        "\n",
        "* Antes que cualquier cosa, **cree una copia de este notebook: Click en *File*, luego *Save a Copy in Drive***\n",
        "* Renombre el archivo con el siguiente formato: APELLIDO_NOMBRE_LEGAJO_titulonotebook.ipynb \n",
        "Ejemplo: DIEDRICHS_ANA_99999_FrozenLake.ipynb\n",
        "* Use el notebook, complete las actividades y consignas que se elija. \n",
        "* Este laboratorio es una actividad individual.\n",
        "* Se fomenta el intercambio de opiniones en clase.\n",
        "\n",
        "**ENTREGA**\n",
        "\n",
        "* Una vez finalizado el laboratorio, complete [el formulario de entrega](https://forms.gle/AqdeVPA38chsJqR99) indicando\n",
        " * Apellido\n",
        " * Nombre\n",
        " * Nro Legajo\n",
        " * Turno (tarde o noche)\n",
        " * link de su notebook. El mismo se obtiene si realiza click en *Share* (esquina superior derecha) y luego en *Get shareable link* \n",
        " \n",
        " \n",
        " No se aceptarán otras formas de entrega distintas a la mencionada.\n",
        " \n",
        " Fecha límite de entrega: se indicará en clase o por la lista de correos. Última semana de clases es la entrega."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc774v7nST24",
        "colab_type": "text"
      },
      "source": [
        "## Instalando y cargando librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihUm___JAxIH",
        "colab_type": "text"
      },
      "source": [
        "Instamos y cargamos librerias que vamos a usar en este ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C08Vwyo49bon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cmake 'gym' scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBoTpK0bAazf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gym.spaces as spaces\n",
        "import time\n",
        "from time import sleep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-yosvkLTCc4",
        "colab_type": "text"
      },
      "source": [
        "## Problema FrozenLake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59FzNisRTt9g",
        "colab_type": "text"
      },
      "source": [
        "Trabajaremos con el ambiente FrozenLake o lago congelado. El objetivo del agente es ir de un origen a un destino sin caer en agujeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAUoTmAuDevj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('FrozenLake-v0') # Trabajaremos con el ambiente FrozenLake\n",
        "env.seed(1234) # semilla para generador random\n",
        "env.render() # nos muestra una imagen de como luce el ambiente."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhTEppInTHS_",
        "colab_type": "text"
      },
      "source": [
        "### ACTIVIDAD \n",
        "\n",
        "Leyendo [la documentación](https://gym.openai.com/envs/FrozenLake-v0/) del ambiente FrozenLake\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVTWJQnYRkt5",
        "colab_type": "text"
      },
      "source": [
        "#### 1.a) ESTADOS: Explique el ambiente, qué significan la F, H G y S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zKlotkVTkwK",
        "colab_type": "text"
      },
      "source": [
        "**SU RESPUESTA AQUI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKhKTwfkUI8S",
        "colab_type": "text"
      },
      "source": [
        "#### 1.b) ACCIONES: El siguiente código nos muestra el espacio de acciones del agente. ¿Cuáles son las cuatro acciones que puede realizar el agente?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab1QutmUUJzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm8rzF4tVHy2",
        "colab_type": "text"
      },
      "source": [
        "**SU RESPUESTA AQUI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyJ0TWyZ2Ub",
        "colab_type": "text"
      },
      "source": [
        "#### 1.c) ¿Cuáles son los valores de recompensa/penalización y cuándo la recibe? Explique\n",
        "\n",
        "**SU RESPUESTA AQUI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_dWzxTGLrqm",
        "colab_type": "text"
      },
      "source": [
        "## Tabla de probabilidades de transición estado - acción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqRpbSQlQo0M",
        "colab_type": "text"
      },
      "source": [
        "![Imagen dibujo de los estados](https://twice22.github.io/images/rl_series/frozenlake.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQ0JnLBNquj",
        "colab_type": "text"
      },
      "source": [
        "El atributo env.P del objeto *environment* de Open AI Gym guarda la tabla de transición de probabilidades estado-acción, o función de transición."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aSlfhLyNqJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHi8D1JdL53M",
        "colab_type": "text"
      },
      "source": [
        "Para el estado 0, vemos las probabilidades de transición"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2dRb48L3QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P[14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxKSD_IwMAu-",
        "colab_type": "text"
      },
      "source": [
        "Si el estado es 0 y la acción es 2 (ir a la derecha)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xIzyulL-3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P[0][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvc_Ra9MPND",
        "colab_type": "text"
      },
      "source": [
        "El primer número, 0.33 es la probabilidad de transición.\n",
        "\n",
        "El segundo, 4, es el siguiente estado, \n",
        "\n",
        "El tercero la recompensa, 0 en este caso.\n",
        "\n",
        "El cuarto ítem, False, es si finaliza el episodio.\n",
        "\n",
        "Note que estando en el estado 0 y ejecutando la acción 2 el agente tiene iguales probabilidades de permanecer en el mismo estado (el cero), pasar al estado 1 o pasar al estado 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yztQnXPRKfQ",
        "colab_type": "text"
      },
      "source": [
        "### Actividad ¿qué interpreta de la siguiente línea de código? Indique desde qué estado se parte, la acción que se ejecuta y qué significa la salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LCS0vD0NeMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P[6][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niqhAaCSQ8-P",
        "colab_type": "text"
      },
      "source": [
        "Si estoy en el estado 15 y ejecuto la acción 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPOiLNMlQ8HO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P[14][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDlA344YbNME",
        "colab_type": "text"
      },
      "source": [
        "### Actividad: ¿qué observa de los valores de probabilidad de la tabla env.env.P o $T(s,a,s')$ ? ¿qué podemos decir del agente/entorno?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktH5An7jOCh5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSXvp9TFSaP-",
        "colab_type": "text"
      },
      "source": [
        "## Agente aleatorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2L9j4gZqWz",
        "colab_type": "text"
      },
      "source": [
        "Creamos una clase, RandomAgent, que sería un agente aleatorio, es decir, la siguiente acción a tomar la elige al azar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TO8xg4AeAh-x",
        "colab": {}
      },
      "source": [
        "class RandomAgent():\n",
        "\tdef __init__(self,action_space):\n",
        "\t\tself.action_space = action_space\n",
        "\t\n",
        "\tdef elegir_accion(self,observation):\n",
        "\t\t## Regresa una acción aleatoria \n",
        "\t\treturn self.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hddUEhXJG3Wx",
        "colab_type": "text"
      },
      "source": [
        "Observe y ejecute la siguiente línea varias veces. ¿Qué regresa elegir_accion?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XObrSfofG3fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agente = RandomAgent(env.action_space)\n",
        "obs=env.reset()\n",
        "env.seed(1234) \n",
        "agente.elegir_accion(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ_9IDLkD-aD",
        "colab_type": "text"
      },
      "source": [
        "## Experimento con el agente aleatorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdloPqpfX7df",
        "colab_type": "text"
      },
      "source": [
        "Función que define si cayó en un hoyo o no.\n",
        "Podría modificarla si lo desea o fuera necesario. \n",
        "Interprete su funcionamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSpadRKPX6qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def es_un_pozo(obs,reward,done):\n",
        "  es_pozo = False\n",
        "  if(reward <= 0 and done == True):\n",
        "    es_pozo = True\n",
        "  \n",
        "  return es_pozo\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHFzec80EAfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = RandomAgent(env.action_space)\n",
        "\n",
        "all_rewards=[] # guardamos recompensas\n",
        "frames=[] # para guardar los frames y luego mostrar el comportamiento del agente\n",
        "contador=0 # contador de acciones o pasos datos en total\n",
        "caidas_al_lago=0\n",
        "\n",
        "for _ in range(1000):\n",
        "\n",
        "  obs=env.reset()\n",
        "  total_reward = 0\n",
        "\t\n",
        "  while True:\n",
        "    \n",
        "    action = agent.elegir_accion(obs)\n",
        "    obs,reward,done,info = env.step(action) # la acción se ejecuta en el ambiente\n",
        "    contador+=1\n",
        "    \n",
        "    if(es_un_pozo(obs,reward,done)): \n",
        "      caidas_al_lago+=1\n",
        "    \n",
        "    # para renderizar\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': obs,\n",
        "        'action': action,\n",
        "        'reward': reward \n",
        "    })\n",
        "    \n",
        "    if done: # si terminó el episodio (llega al objetivo o cae en un hoyo)\n",
        "      all_rewards.append(reward) # guarda la recompensa\n",
        "      break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBMQDR5ifTg",
        "colab_type": "text"
      },
      "source": [
        "Ejecute el siguiente bloque de código para visualizar gráficamente el comportamiento del agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjFits1jk3CG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(frames) # cantidad total de frames, o acciones a mostrar del agente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBTkn6keWlWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "# descomente la siguiente línea        \n",
        "#print_frames(frames) #si desea ver todos los frames, puede demorar bastante"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLFkHYG4-s5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxIleuuKEPus",
        "colab_type": "text"
      },
      "source": [
        "Recompensa promedio de agente aleatorio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAuLngDTEToy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(all_rewards)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLx0maG-_DBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Recompensa promedio: \", np.mean(all_rewards))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76oX25hriowM",
        "colab_type": "text"
      },
      "source": [
        "#### 1.d) Cuántas veces llegó al objetivo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC15Uci6jQn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calcule lo que responde a la pregunta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfHZJU2jER34",
        "colab_type": "text"
      },
      "source": [
        "Evaluacion de la recompensa en el tiempo de la recompensa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIJQTMgkEiTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(all_rewards)\n",
        "# variacion de los valores por cada iteracion\n",
        "plt.plot(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHtOfc4Trxlz",
        "colab_type": "text"
      },
      "source": [
        "Recompensa acumulada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVAKTaztEyoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recompensa acumulada\n",
        "plt.plot(df.cumsum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xJ2Ne9sSh6e",
        "colab_type": "text"
      },
      "source": [
        "## Iteración de valores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cM6j4s4Z3pl",
        "colab_type": "text"
      },
      "source": [
        "Creamos una clase que represente un agente que ejecuta iteración de valores, es la clase ValueIterAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6swpkboEfud",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/waqasqammar/MDP-with-Value-Iteration-and-Policy-Iteration/raw/d66dfe7aad0d23d7956aa87f05989a66a15202f6/nb_images/value_iter.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji8jLaIUDPDl",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.dropbox.com/s/ix76ita6h3igdde/iteracion-de-valores.jpg?dl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs1V2qlBNkLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
        "    \"\"\"\n",
        "    Helper function to  calculate state-value function\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object\n",
        "        state: state to consider\n",
        "        V: Estimated Value for each state. Vector of length nS\n",
        "        discount_factor: MDP discount factor\n",
        "        \n",
        "    Return:\n",
        "        action_values: Expected value of each action in a state. Vector of length nA\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize vector of action values\n",
        "    action_values = np.zeros(env.nA)\n",
        "    \n",
        "    # loop over the actions we can take in an enviorment \n",
        "    for action in range(env.nA):\n",
        "        # loop over the P_sa distribution.\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
        "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmDnni7v1w0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "class ValueIterAgent(): # clase ValueIterAgent\n",
        "  #función constructora\n",
        "\tdef __init__(self,env,gamma):\n",
        "\t\tself.max_iterations = 1000\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.num_estados=env.observation_space.n\n",
        "\t\tself.num_actions=env.action_space.n\n",
        "\t\tself.state_prob = env.env.P # tabla T(s,a,s')\n",
        "\t\tself.values = np.zeros(env.observation_space.n) # valores de utilidad de los estados\n",
        "\t\tself.policy = np.zeros(env.observation_space.n) # política\n",
        "    \n",
        "  #función de iteración de valores\n",
        "\tdef value_iteration(self):\n",
        "\t    for i in range(self.max_iterations): # hasta un nro máximo de iteraciones\n",
        "\t        prev_v = np.copy(self.values)\n",
        "\t        for state in range(self.num_estados): # por cada estado\n",
        "\t            Q_value = []\n",
        "\t            for action in range(self.num_actions): # por cada acción \n",
        "\t                next_states_rewards = []\n",
        "                  # dado un estado y una acción, por cada tupla:\n",
        "                  # P(transicion de estado) + próximo estado + recompensa\n",
        "\t                for trans_prob, next_state, reward_prob, _ in self.state_prob[state][action]: \n",
        "\t                   \n",
        "\t                    next_states_rewards.append((trans_prob * (reward_prob + self.gamma * prev_v[next_state]))) \n",
        "                      \n",
        "\t                \n",
        "\t                Q_value.append(sum(next_states_rewards)) # suma de todas las recompensas futuras\n",
        "                  \n",
        "\t            self.values[state] = max(Q_value) # asignar la mayor utilidad futura para ese estado\n",
        "\t            #print(state,Q_value)\n",
        "\t            #sleep(5)\n",
        "\t    return self.values\n",
        "  \n",
        "  #función de extraer la política\n",
        "\tdef extract_policy(self):\n",
        "\t   \n",
        "\t    for s in range(self.num_estados):\n",
        "\t        q_sa = np.zeros(self.num_actions)\n",
        "\t        for a in range(self.num_actions):\n",
        "\t            for prox_estado in self.state_prob[s][a]:\n",
        "                \n",
        "\t                # next_sr es una tupla de (probability, next state, reward, done)\n",
        "\t                p, s_, r, _ = prox_estado\n",
        "                  \n",
        "\t                q_sa[a] += (p * (r + self.gamma * self.values[s_]))\n",
        "                  \n",
        "\t        self.policy[s] = np.argmax(q_sa)   \n",
        "          \n",
        "  # regresa una acción dada una observación del ambiente, según la política\n",
        "\tdef choose_action(self,observation):\n",
        "\t\t\t\n",
        "\t\treturn self.policy[observation]  \t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCj88KI_Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.env.P[14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvSArEFuEJB3",
        "colab_type": "text"
      },
      "source": [
        "## Agente con iteración de valores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URM0CJWoex_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 1\n",
        "\n",
        "agent = ValueIterAgent(env,gamma)\n",
        "\n",
        "agent.value_iteration();\n",
        "agent.extract_policy();\n",
        "\n",
        "print( \"La política del agente: \", agent.policy)\n",
        "all_rewards=[]\n",
        "frames=[]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oDzYop-O1Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Los valores de Utilidades son \\n\",agent.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNmRhb8OTHGb",
        "colab_type": "text"
      },
      "source": [
        "Como es un arreglo, lo formateo a una dimension 4x4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOy2ojyeR-0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.values.reshape(4,4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wul3nR_gO3GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(agent.values.reshape(4,4),  cmap=\"YlGnBu\", annot=True, cbar=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BebGhGeSR8MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 1\n",
        "\n",
        "agent = ValueIterAgent(env,gamma)\n",
        "\n",
        "agent.value_iteration();\n",
        "agent.extract_policy();\n",
        "\n",
        "print( \"La política del agente: \", agent.policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAG43X7slhu8",
        "colab_type": "text"
      },
      "source": [
        "### Actividad 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kzuwhhoSJPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.extract_policy();\n",
        "\n",
        "print( \"La política del agente: \", agent.policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDp2ZyY8SEMx",
        "colab_type": "text"
      },
      "source": [
        "#### 2.a) Dibuje  la política que toma el agente sobre los estados posibles\n",
        "\n",
        "Ejemplo de un dibujito de los estados y políticas\n",
        "\n",
        "|  |    | ||\n",
        "|------|------|------|------|\n",
        "|S derecha |F abajo |F izquierda |F abajo|\n",
        "|F derecha |H abajo |F izquierda |H izquierda|\n",
        "|F derecha |F abajo |F izquierda |H izquierda|\n",
        "|H derecha |F abajo |F izquierda |G arriba |\n",
        "\n",
        "Otro ejemplo (queda a su imaginación los símbolos o representación :-) )\n",
        "\n",
        "\n",
        "|  |    | ||\n",
        "|------|------|------|------|\n",
        "|S <- |F v |F <- |F v|\n",
        "|F &#8594; |H &#8595; |F &#8592; |H &#8592;|\n",
        "|F derecha |F abajo |F izquierda |H izquierda|\n",
        "|H derecha |F &#8595; |F izquierda |G &#8593;  |\n",
        "\n",
        "\n",
        "Ud. debe interpretar el resultado de la salida de ejecutar la línea \n",
        "\n",
        "```\n",
        "print( \"La política del agente: \", agent.policy)\n",
        "```\n",
        "\n",
        "#### 2.b) Actividad\n",
        "\n",
        "¿Qué interpreta de la política obtenida? (cuadro anterior) Cuente lo que ve, detalle y explique con su intuición"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxI7bWGuU5lv",
        "colab_type": "text"
      },
      "source": [
        "**su respuesta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwQgX1EvqFY1",
        "colab_type": "text"
      },
      "source": [
        "### Actividad 3 experimento iteración de valores\n",
        "\n",
        "Corra el siguiente script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGT-F3l83PtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 1\n",
        "\n",
        "agent = ValueIterAgent(env,gamma)\n",
        "\n",
        "agent.value_iteration();\n",
        "\n",
        "agent.extract_policy();\n",
        "\n",
        "print( \"La política del agente: \", agent.policy)\n",
        "all_rewards=[]\n",
        "frames=[]\n",
        "\n",
        "caidas_al_lago=0\n",
        "\n",
        "for _ in range(1000):\n",
        "\n",
        "  obs=env.reset()\n",
        "  total_reward = 0\n",
        "\t\n",
        "  while True:\n",
        "\n",
        "    action = agent.choose_action(obs)\n",
        "    obs,reward,done,info = env.step(action)\n",
        "\n",
        "    if(es_un_pozo(obs,reward,done)): \n",
        "      caidas_al_lago+=1\n",
        "\n",
        "    # descomente la siguiente línea si desea guardarl os frames para luego visualizar el comportamiento del agente\n",
        "    \n",
        "    #frames.append({'frame': env.render(mode='ansi'),'state': obs,'action': action,'reward': reward })\n",
        "\n",
        "    if done:\n",
        "      all_rewards.append(reward)\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAiS3JCAF61",
        "colab_type": "text"
      },
      "source": [
        "Añada y ejecute los bloques de código y bloques de texto para responder:\n",
        "\n",
        "* Cuántas veces llegó al objetivo\n",
        "* Recompensa promedio del experimento\n",
        "* Gráfica der recompensa acumulada\n",
        "* Cuántas veces se cayó al lago"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsLEOnp8WqQ7",
        "colab_type": "text"
      },
      "source": [
        "### Actividad 4, medir tiempo de ejecución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpF3M90nYk-J",
        "colab_type": "text"
      },
      "source": [
        "El siguiente script es un ejemplo de como medir el tiempo de duración de un proceso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXu4bIFKWtNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tic = time.time() # estampa inicial de tiempo. \n",
        "# llame a las funciones que desea ejecutar\n",
        "sleep(5) # la función sleep el valor es en segundos\n",
        "toc = time.time() # estampa final de tiempo\n",
        "# calculo la diferencia entre valor final e inicio de tiempo\n",
        "duración_total = (toc - tic) * 1000 # *1000 para ver el tiempo en milisegundos\n",
        "duración_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buqfWIb7Y9gh",
        "colab_type": "text"
      },
      "source": [
        "Agrege a los scripts anteriores: agente aleatorio y agente por iteración de valores las mediciones de tiempo total de ejecución."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkjiXsTsTbV",
        "colab_type": "text"
      },
      "source": [
        "### Actividad 5 \n",
        "\n",
        "Modifique el valor de gamma del experimento de iteración por valores a 0.7 ( u otro valor menor a 1). \n",
        "\n",
        "¿Cuál es el valor de recompensa promedio obtenida?\n",
        "\n",
        "¿Es mejor o peor el compotamiento del agente respecto al experimento con gamma = 1 ? \n",
        "\n",
        "¿Por qué lo cree? Justifique\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWxEVF9Pr-MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copie y pegue código para reproducir el experimento de iteración de valores con gamma a 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnQ88P_YaT7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSHj-2_chxOB",
        "colab_type": "text"
      },
      "source": [
        "## Iteración de políticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTU1_a-viEW-",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.dropbox.com/s/gi0nnc1pgq2ug28/iteracion-de-politicas-.jpg?dl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSTDSHYHiBYc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/waqasqammar/MDP-with-Value-Iteration-and-Policy-Iteration/raw/d66dfe7aad0d23d7956aa87f05989a66a15202f6/nb_images/policy_iter.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWVxbHVVmLUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_lookahead(env, state, V , discount_factor = 0.99):\n",
        "    \"\"\"\n",
        "    Función ayuda o auxiliar para calcular estado-valor \n",
        "    \n",
        "    Arguments:\n",
        "        env: objeto del ambiente openAI GYM \n",
        "        state: estado a considerar\n",
        "        V: valor estimado para cada estado. Vector de longitud nS.\n",
        "        discount_factor: factor de descuento MDP.\n",
        "        \n",
        "    Return:\n",
        "        action_values: Valor esperado por cada acción en un estado. Vector de longitud nA\n",
        "    \"\"\"\n",
        "    \n",
        "    # inicitaliza el vector de valores d acciones\n",
        "    action_values = np.zeros(env.nA)\n",
        "    \n",
        "    # itera sobre las acciones que podemos tomar del ambiente\n",
        "    for action in range(env.nA):\n",
        "        # itera sobre la distribución P_sa (probabilidad estado/acción)\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
        "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS-cgT9imE_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(env, policy, V, discount_factor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Helper function to update a given policy based on given value function.\n",
        "      \n",
        "    \n",
        "    Arguments:\n",
        "        env: objeto del ambiente openAI GYM \n",
        "        state: estado a considerar\n",
        "        V: valor estimado para cada estado. Vector de longitud nS.\n",
        "        discount_factor: factor de descuento MDP.\n",
        "    Return:\n",
        "        policy: Política actualizada basada en la función estado-Valor o estado-Utilidad 'V'.\n",
        "    \"\"\"\n",
        "    \n",
        "    for state in range(env.nS):\n",
        "        # Dado un estado, computar el valor estado-acción.\n",
        "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "        \n",
        "        # elige una acción que maximice el valor de utilidad de estado-acción.\n",
        "        policy[state] =  np.argmax(action_values)\n",
        "        \n",
        "    return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldLpO_UaNujr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        optimal_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize value fucntion\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # iterate over max_iterations\n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        #  keep track of change with previous value function\n",
        "        prev_v = np.copy(V) \n",
        "    \n",
        "        # loop over all states\n",
        "        for state in range(env.nS):\n",
        "            \n",
        "            # Asynchronously update the state-action value\n",
        "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "            \n",
        "            # Synchronously update the state-action value\n",
        "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
        "            \n",
        "            # select best action to perform based on highest state-action value\n",
        "            best_action_value = np.max(action_values)\n",
        "            \n",
        "            # update the current state-value fucntion\n",
        "            V[state] =  best_action_value\n",
        "            \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            # if values of 'V' not changing after one iteration\n",
        "            if (np.all(np.isclose(V, prev_v))):\n",
        "                print('Value converged at iteration %d' %(i+1))\n",
        "                break\n",
        "\n",
        "    # intialize optimal policy\n",
        "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "    # update the optimal polciy according to optimal value function 'V'\n",
        "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "    return V, optimal_policy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0jzZb-vN5jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "tic = time.time()\n",
        "opt_V, opt_Policy = value_iteration(env.env, max_iteration = 1000)\n",
        "toc = time.time()\n",
        "elapsed_time = (toc - tic) * 1000\n",
        "print (f\"Time to converge: {elapsed_time: 0.3} ms\")\n",
        "print('Optimal Value function: ')\n",
        "print(opt_V.reshape((4, 4)))\n",
        "print('Final Policy: ')\n",
        "print(opt_Policy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZQ04HwhzV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_eval(env, policy, V, discount_factor):\n",
        "    \"\"\"\n",
        "    Helper function to evaluate a policy.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        policy: policy to evaluate.\n",
        "        V: Estimated Value for each state. Vector of length nS.\n",
        "        discount_factor: MDP discount factor.\n",
        "    Return:\n",
        "        policy_value: Estimated value of each state following a given policy and state-value 'V'. \n",
        "        \n",
        "    \"\"\"\n",
        "    policy_value = np.zeros(env.nS)\n",
        "    for state, action in enumerate(policy):\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "            policy_value[state] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return policy_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irfIzDG0h1Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        new_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize the state-Value function\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # intialize a random policy\n",
        "    policy = np.random.randint(0, 4, env.nS)\n",
        "    policy_prev = np.copy(policy)\n",
        "    \n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        # evaluate given policy\n",
        "        V = policy_eval(env, policy, V, discount_factor)\n",
        "        \n",
        "        # improve policy\n",
        "        policy = update_policy(env, policy, V, discount_factor)\n",
        "        \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            if (np.all(np.equal(policy, policy_prev))):\n",
        "                print('policy converged at iteration %d' %(i+1))\n",
        "                break\n",
        "            policy_prev = np.copy(policy)\n",
        "            \n",
        "\n",
        "            \n",
        "    return V, policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwu7qeFSlseJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "enviorment2 = gym.make('FrozenLake-v0')\n",
        "tic = time.time()\n",
        "opt_V2, opt_policy2 = policy_iteration(enviorment2.env, discount_factor = 0.999, max_iteration = 10000)\n",
        "toc = time.time()\n",
        "elapsed_time = (toc - tic) * 1000\n",
        "print (f\"Tiempo para converger: {elapsed_time: 0.3} ms\")\n",
        "print('Función de valores/utilidades óptima: ')\n",
        "print(opt_V2.reshape((4, 4)))\n",
        "print('Política final: ')\n",
        "print(opt_policy2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjydv4A0qh75",
        "colab_type": "text"
      },
      "source": [
        "### Actividad 4\n",
        "\n",
        "Escriba sus conclusiones y comparaciones entre el experimento del agente con comportamiento aleatorio, el de iteración por valores y el de iteración de políticas\n",
        "\n",
        "\n",
        "**SU RESPUESTA AQUI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWpYqOdOinLS",
        "colab_type": "text"
      },
      "source": [
        "### ACTIVIDAD 5 - Agrandando el ambiente "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sczSqPdi6-2",
        "colab_type": "text"
      },
      "source": [
        "En los ejercicios anteriores el ambiente tenía 16 estados, un espacio de 4x4 celdas donde se movía el agente. Ahora pasamos a un espacio de 8x8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-rZu55ciqXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name  = 'FrozenLake8x8-v0'\n",
        "gamma = 1.0\n",
        "env = gym.make(env_name)\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4QiiEwZdzO",
        "colab_type": "text"
      },
      "source": [
        "Ejecute para este nuevo ambiente un agente aleatorio, otro por iteración de políticas y otro por iteración de valores. \n",
        "\n",
        "Copie y pegue, reutilice código anterior. Añada todos los bloques de código que crea necesario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfOmrrA3ZlJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-yxF_0FUEdm",
        "colab_type": "text"
      },
      "source": [
        "#### ACTIVIDAD 5.1 Conclusiones\n",
        "\n",
        "Escriba conclusiones generales sobre los experimentos realizados en la actividad anterior.\n",
        "\n",
        "Compare. ¿Qué cambios evidencia de pasar de un espacio de estados 4x4 a 8x8? Explique, detalle, conteste lo más completo posible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4x6i3MBSLqo",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxJuLhcYjlnE",
        "colab_type": "text"
      },
      "source": [
        "* Reinforcement Learning with OpenAI Gym - Value Iteration Frozen Lake - Code Heroku - [enlace](https://www.slideshare.net/codeheroku/reinforcement-learning-with-openai-gym-value-iteration-frozen-lake-code-heroku)\n",
        "*  Frozen Lake environment in Open AI Gym [enlace a código del repo](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py)\n",
        "* Open AI Gym (https://gym.openai.com/)\n",
        "* Iteración de valores, blog, por Holly Grimm  https://hollygrimm.com/rl_mdp\n",
        "* Victor Busa, Open AI Gym intro, https://twice22.github.io/rl-part1/"
      ]
    }
  ]
}